{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865ba2a3-bdba-4404-8722-31a271a87fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-08 16:26:20.159002: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749399980.308592    2699 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749399980.362893    2699 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1749399980.740061    2699 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749399980.740209    2699 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749399980.740222    2699 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749399980.740235    2699 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-08 16:26:20.781898: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Information:\n",
      "Number of training examples: 75\n",
      "Number of validation examples: 9\n",
      "tokenizer.pad_token_id=0\n",
      "tokenizer.eos_token_id=1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e33d47807fb463d934ac2dcbd9c83b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regex_pattern='(?:.*?(?:language|text).*?(?:self_attn|attention|attn|mlp|feed_forward|ffn|dense).*?(?:q_proj|k_proj|v_proj|o_proj|gate_proj|up_proj|down_proj).*?)|(?:\\\\bmodel\\\\.layers\\\\.[\\\\d]{1,}\\\\.(?:self_attn|attention|attn|mlp|feed_forward|ffn|dense)\\\\.(?:(?:q_proj|k_proj|v_proj|o_proj|gate_proj|up_proj|down_proj)))'\n",
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): Gemma2ForCausalLM(\n",
      "      (model): Gemma2Model(\n",
      "        (embed_tokens): Embedding(256000, 3584, padding_idx=0)\n",
      "        (layers): ModuleList(\n",
      "          (0-41): 42 x Gemma2DecoderLayer(\n",
      "            (self_attn): Gemma2Attention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=3584, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3584, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=3584, out_features=2048, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3584, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=3584, out_features=2048, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3584, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=3584, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=3584, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (mlp): Gemma2MLP(\n",
      "              (gate_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=3584, out_features=14336, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3584, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=14336, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=3584, out_features=14336, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3584, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=14336, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=14336, out_features=3584, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=14336, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=3584, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (act_fn): PytorchGELUTanh()\n",
      "            )\n",
      "            (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "            (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "            (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "            (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "          )\n",
      "        )\n",
      "        (norm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "        (rotary_emb): Gemma2RotaryEmbedding()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=3584, out_features=256000, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f18cee29d4f4b74a1c2b1b41f86409a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting train dataset to ChatML:   0%|          | 0/75 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77d608ec8e3945cebc6844d5295c2071",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset:   0%|          | 0/75 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f221bc796984a05bf699a825ab861ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/75 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14fed3a257ab4c87a57d149fa47d3caf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/75 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3ef5dcc571d4a86a5711cd87f1f0d48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting eval dataset to ChatML:   0%|          | 0/9 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47fd4aebcc1a409397483462e35e2f73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to eval dataset:   0%|          | 0/9 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee1ee0a9b4ab42de9093f74c03f72623",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/9 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1deaa69c9c36437d8b2ba152d8794aed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/9 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='300' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [300/750 04:24 < 06:39, 1.13 it/s, Epoch 4/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.999500</td>\n",
       "      <td>1.232337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.054200</td>\n",
       "      <td>1.208798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.699200</td>\n",
       "      <td>1.435191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.397600</td>\n",
       "      <td>1.807504</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping triggered after 2 evaluations without improvement\n"
     ]
    }
   ],
   "source": [
    "# https://ai.google.dev/gemma/docs/core/huggingface_text_finetune_qlora\n",
    "# https://huggingface.co/blog/gemma-peft\n",
    "#took this from https://github.com/EmilRyd/eliciting-secrets/tree/main?tab=readme-ov-file\n",
    "import argparse\n",
    "import os\n",
    "import re\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import HfApi, create_repo\n",
    "from omegaconf import OmegaConf\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainerCallback,\n",
    ")\n",
    "from trl import DataCollatorForCompletionOnlyLM, SFTConfig, SFTTrainer\n",
    "\n",
    "import wandb\n",
    "\n",
    "\n",
    "def apply_chat_template(example, tokenizer):\n",
    "    mesages = tokenizer.apply_chat_template(\n",
    "        example[\"messages\"],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False,\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "    return {\"text\": mesages}\n",
    "\n",
    "\n",
    "def tokenize(example, tokenizer):\n",
    "    processed = tokenizer(example[\"text\"])\n",
    "    if (\n",
    "        tokenizer.eos_token_id is not None\n",
    "        and processed[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
    "    ):\n",
    "        processed[\"input_ids\"] = processed[\"input_ids\"] + [tokenizer.eos_token_id]\n",
    "        processed[\"attention_mask\"] = processed[\"attention_mask\"] + [1]\n",
    "    return processed\n",
    "\n",
    "\n",
    "def tokenize_with_chat_template(dataset, tokenizer):\n",
    "    \"\"\"Tokenize example with chat template applied.\"\"\"\n",
    "    dataset = dataset.map(apply_chat_template, fn_kwargs={\"tokenizer\": tokenizer})\n",
    "\n",
    "    dataset = dataset.map(tokenize, fn_kwargs={\"tokenizer\": tokenizer})\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def upload_to_hub(model_path, repo_id, subfolder_name, hf_token):\n",
    "    \"\"\"Upload the fine-tuned model to a specific subfolder in the Hugging Face Hub.\"\"\"\n",
    "    target_repo = f\"{repo_id}/{subfolder_name}\"\n",
    "    print(f\"\\nUploading model to {target_repo}...\")\n",
    "\n",
    "    # Create repository if it doesn't exist (base repo)\n",
    "    try:\n",
    "        create_repo(repo_id, token=hf_token, exist_ok=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating base repository {repo_id}: {e}\")\n",
    "        # Continue attempting upload, maybe repo exists but creation check failed\n",
    "\n",
    "    # Upload model files to the subfolder\n",
    "    api = HfApi()\n",
    "    try:\n",
    "        api.upload_folder(\n",
    "            folder_path=model_path,\n",
    "            repo_id=repo_id,\n",
    "            # path_in_repo=subfolder_name,  # Specify the subfolder here\n",
    "            token=hf_token,\n",
    "            repo_type=\"model\",\n",
    "        )\n",
    "        print(f\"Successfully uploaded model to {target_repo}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading model to {target_repo}: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "class WandbLoggingCallback(TrainerCallback):\n",
    "    def __init__(self, trainer=None):\n",
    "        self.step = 0\n",
    "        self.trainer = trainer\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs is not None and wandb.run is not None:\n",
    "            # Log training metrics\n",
    "            metrics = {\n",
    "                \"train/loss\": logs.get(\"loss\", None),\n",
    "                \"train/learning_rate\": logs.get(\"learning_rate\", None),\n",
    "            }\n",
    "            # Remove None values\n",
    "            metrics = {k: v for k, v in metrics.items() if v is not None}\n",
    "            if metrics:\n",
    "                wandb.log(metrics, step=self.step)\n",
    "                self.step += 1\n",
    "\n",
    "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
    "        if metrics is not None and wandb.run is not None:\n",
    "            # Log evaluation metrics\n",
    "            eval_metrics = {\n",
    "                \"eval/loss\": metrics.get(\"eval_loss\", None),\n",
    "                \"eval/epoch\": metrics.get(\"epoch\", None),\n",
    "            }\n",
    "            # Remove None values\n",
    "            eval_metrics = {k: v for k, v in eval_metrics.items() if v is not None}\n",
    "            if eval_metrics:\n",
    "                wandb.log(eval_metrics, step=self.step)\n",
    "\n",
    "\n",
    "class EarlyStoppingCallback(TrainerCallback):\n",
    "    def __init__(self, early_stopping_patience=3):\n",
    "        self.early_stopping_patience = early_stopping_patience\n",
    "        self.best_eval_loss = float(\"inf\")\n",
    "        self.patience_counter = 0\n",
    "\n",
    "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
    "        if metrics is not None:\n",
    "            eval_loss = metrics.get(\"eval_loss\", None)\n",
    "            if eval_loss is not None:\n",
    "                if eval_loss < self.best_eval_loss:\n",
    "                    self.best_eval_loss = eval_loss\n",
    "                    self.patience_counter = 0\n",
    "                else:\n",
    "                    self.patience_counter += 1\n",
    "                    if self.patience_counter >= self.early_stopping_patience:\n",
    "                        print(\n",
    "                            f\"\\nEarly stopping triggered after {self.early_stopping_patience} evaluations without improvement\"\n",
    "                        )\n",
    "                        control.should_training_stop = True\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--config\", type=str, default=\"config.yaml\", help=\"Path to config file\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--train_data\", type=str, help=\"Override train data path from config\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--test_data\", type=str, help=\"Override test data path from config\"\n",
    "    )\n",
    "    parser.add_argument(\"--env\", type=str, default=\".env\", help=\"Path to .env file\")\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "def load_environment(args):\n",
    "    # Load environment variables\n",
    "    if not os.path.exists(args.env):\n",
    "        raise FileNotFoundError(f\"Environment file not found: {args.env}\")\n",
    "\n",
    "    load_dotenv(args.env)\n",
    "\n",
    "    # Check for required environment variables\n",
    "    required_vars = [\"HF_TOKEN\"]\n",
    "    missing_vars = [var for var in required_vars if not os.getenv(var)]\n",
    "    if missing_vars:\n",
    "        raise ValueError(\n",
    "            f\"Missing required environment variables: {', '.join(missing_vars)}\"\n",
    "        )\n",
    "\n",
    "    return {\n",
    "        \"hf_token\": os.getenv(\"HF_TOKEN\"),\n",
    "        \"wandb_api_key\": os.getenv(\"WANDB_API_KEY\"),\n",
    "    }\n",
    "\n",
    "\n",
    "def get_peft_regex(\n",
    "    model,\n",
    "    finetune_vision_layers: bool = True,\n",
    "    finetune_language_layers: bool = True,\n",
    "    finetune_attention_modules: bool = True,\n",
    "    finetune_mlp_modules: bool = True,\n",
    "    target_modules: list[str] = None,\n",
    "    vision_tags: list[str] = [\n",
    "        \"vision\",\n",
    "        \"image\",\n",
    "        \"visual\",\n",
    "        \"patch\",\n",
    "    ],\n",
    "    language_tags: list[str] = [\n",
    "        \"language\",\n",
    "        \"text\",\n",
    "    ],\n",
    "    attention_tags: list[str] = [\n",
    "        \"self_attn\",\n",
    "        \"attention\",\n",
    "        \"attn\",\n",
    "    ],\n",
    "    mlp_tags: list[str] = [\n",
    "        \"mlp\",\n",
    "        \"feed_forward\",\n",
    "        \"ffn\",\n",
    "        \"dense\",\n",
    "    ],\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Create a regex pattern to apply LoRA to only select layers of a model.\n",
    "    \"\"\"\n",
    "    if not finetune_vision_layers and not finetune_language_layers:\n",
    "        raise RuntimeError(\n",
    "            \"No layers to finetune - please select to finetune the vision and/or the language layers!\"\n",
    "        )\n",
    "    if not finetune_attention_modules and not finetune_mlp_modules:\n",
    "        raise RuntimeError(\n",
    "            \"No modules to finetune - please select to finetune the attention and/or the mlp modules!\"\n",
    "        )\n",
    "\n",
    "    from collections import Counter\n",
    "\n",
    "    # Get only linear layers\n",
    "    modules = model.named_modules()\n",
    "    linear_modules = [\n",
    "        name for name, module in modules if isinstance(module, torch.nn.Linear)\n",
    "    ]\n",
    "    all_linear_modules = Counter(x.rsplit(\".\")[-1] for x in linear_modules)\n",
    "\n",
    "    # Isolate lm_head / projection matrices if count == 1\n",
    "    if target_modules is None:\n",
    "        only_linear_modules = []\n",
    "        projection_modules = {}\n",
    "        for j, (proj, count) in enumerate(all_linear_modules.items()):\n",
    "            if count != 1:\n",
    "                only_linear_modules.append(proj)\n",
    "            else:\n",
    "                projection_modules[proj] = j\n",
    "    else:\n",
    "        assert type(target_modules) is list\n",
    "        only_linear_modules = list(target_modules)\n",
    "\n",
    "    # Create regex matcher\n",
    "    regex_model_parts = []\n",
    "    if finetune_vision_layers:\n",
    "        regex_model_parts += vision_tags\n",
    "    if finetune_language_layers:\n",
    "        regex_model_parts += language_tags\n",
    "    regex_components = []\n",
    "    if finetune_attention_modules:\n",
    "        regex_components += attention_tags\n",
    "    if finetune_mlp_modules:\n",
    "        regex_components += mlp_tags\n",
    "\n",
    "    regex_model_parts = \"|\".join(regex_model_parts)\n",
    "    regex_components = \"|\".join(regex_components)\n",
    "\n",
    "    match_linear_modules = (\n",
    "        r\"(?:\" + \"|\".join(re.escape(x) for x in only_linear_modules) + r\")\"\n",
    "    )\n",
    "    regex_matcher = (\n",
    "        r\".*?(?:\"\n",
    "        + regex_model_parts\n",
    "        + r\").*?(?:\"\n",
    "        + regex_components\n",
    "        + r\").*?\"\n",
    "        + match_linear_modules\n",
    "        + \".*?\"\n",
    "    )\n",
    "\n",
    "    # Also account for model.layers.0.self_attn/mlp type modules like Qwen\n",
    "    if finetune_language_layers:\n",
    "        regex_matcher = (\n",
    "            r\"(?:\"\n",
    "            + regex_matcher\n",
    "            + r\")|(?:\\bmodel\\.layers\\.[\\d]{1,}\\.(?:\"\n",
    "            + regex_components\n",
    "            + r\")\\.(?:\"\n",
    "            + match_linear_modules\n",
    "            + r\"))\"\n",
    "        )\n",
    "\n",
    "    # Check if regex is wrong since model does not have vision parts\n",
    "    check = any(\n",
    "        re.search(regex_matcher, name, flags=re.DOTALL) for name in linear_modules\n",
    "    )\n",
    "    if not check:\n",
    "        regex_matcher = (\n",
    "            r\".*?(?:\" + regex_components + r\").*?\" + match_linear_modules + \".*?\"\n",
    "        )\n",
    "\n",
    "    # Final check to confirm if matches exist\n",
    "    check = any(\n",
    "        re.search(regex_matcher, name, flags=re.DOTALL) for name in linear_modules\n",
    "    )\n",
    "    if not check and target_modules is not None:\n",
    "        raise RuntimeError(\n",
    "            f\"No layers to finetune? You most likely specified target_modules = {target_modules} incorrectly!\"\n",
    "        )\n",
    "    elif not check:\n",
    "        raise RuntimeError(\n",
    "            f\"No layers to finetune for {model.config._name_or_path}. Please file a bug report!\"\n",
    "        )\n",
    "    return regex_matcher\n",
    "\n",
    "\n",
    "\n",
    "    # Parse arguments\n",
    "    #args = parse_args()\n",
    "\n",
    "    # Load environment variables\n",
    "    #env_vars = load_environment(args)\n",
    "\n",
    "    # Load config\n",
    "    #cfg = OmegaConf.load(args.config)\n",
    "\n",
    "    # Extract the word/subfolder name from the output directory\n",
    "   # word = os.path.basename(cfg.training.output_dir)\n",
    "    #if not word:\n",
    "        #print(\n",
    "            #f\"Warning: Could not extract subfolder name from output_dir: {cfg.training.output_dir}. Uploading to base repo.\"\n",
    "        #)\n",
    "        #word = None  # Set word to None if extraction fails\n",
    "\n",
    "    # Load and prepare data\n",
    "if .1 > 0:\n",
    "        dataset = load_dataset(\"json\", data_files=\"dog_and_cat.json\")[\n",
    "            \"train\"\n",
    "        ].train_test_split(test_size=.1)\n",
    "        # manually split into train and test\n",
    "        train_dataset = dataset[\"train\"]\n",
    "        test_dataset = dataset[\"test\"]\n",
    "        print(\"\\nDataset Information:\")\n",
    "        print(f\"Number of training examples: {len(train_dataset)}\")\n",
    "        print(f\"Number of validation examples: {len(test_dataset)}\")\n",
    "else:\n",
    "        train_dataset = load_dataset(\"json\", data_files=cfg.data.train_path)[\"train\"]\n",
    "        test_dataset = None\n",
    "        print(\"\\nDataset Information:\")\n",
    "        print(f\"Number of training examples: {len(train_dataset)}\")\n",
    "        print(\"No validation set (validation_split = 0)\")\n",
    "\n",
    "    # Model and tokenizer setup\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "        \"google/gemma-2-9b-it\", trust_remote_code=True\n",
    "    )\n",
    "tokenizer.add_eos_token = True\n",
    "print(f\"{tokenizer.pad_token_id=}\")\n",
    "print(f\"{tokenizer.eos_token_id=}\")\n",
    "\n",
    "    # Quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,              # turn on 4-bit loading\n",
    "    bnb_4bit_quant_type=\"nf4\",      # use NormalFloat-4 quant format\n",
    "    bnb_4bit_compute_dtype=torch.float16,  # do matmuls in fp16\n",
    ")\n",
    "\n",
    "    # Model kwargs\n",
    "model_kwargs = dict(\n",
    "        attn_implementation=\"eager\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=bnb_config,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    # Load model with quantization and model kwargs\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-9b-it\", **model_kwargs)\n",
    "\n",
    "    # Prepare model for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    # Get regex pattern for LoRA\n",
    "regex_pattern = get_peft_regex(\n",
    "        model,\n",
    "        finetune_vision_layers=False,\n",
    "        finetune_language_layers=True,\n",
    "        finetune_attention_modules=True,\n",
    "        finetune_mlp_modules=True,\n",
    "    )\n",
    "print(f\"{regex_pattern=}\")\n",
    "\n",
    "    # LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "        r=8,\n",
    "        target_modules=regex_pattern,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        lora_dropout=.1,\n",
    "    )\n",
    "\n",
    "    # Get PEFT model\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(model)\n",
    "\n",
    "training_args = SFTConfig(\n",
    "        \n",
    "        num_train_epochs=10,\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=1,\n",
    "        gradient_checkpointing=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        logging_steps= 1,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=False,\n",
    "        bf16=True,\n",
    "        save_strategy=\"epoch\",\n",
    "        max_grad_norm=0.3,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        eval_strategy=\"epoch\"\n",
    "        if .1 > 0\n",
    "        else \"no\",\n",
    "        report_to=\"none\",\n",
    "        #run_name=\"cat_dog\",\n",
    "        load_best_model_at_end=.1 > 0,\n",
    "        metric_for_best_model=\"eval_loss\" if 1.> 0 else None,\n",
    "        greater_is_better=False,\n",
    "        packing=False,\n",
    "        weight_decay=0.01,\n",
    "    )\n",
    "\n",
    "    # Initialize wandb if API key is available\n",
    "    #if env_vars[\"wandb_api_key\"]:\n",
    "        #os.environ[\"WANDB_API_KEY\"] = env_vars[\"wandb_api_key\"]\n",
    "        # Log only essential parameters\n",
    "        #wandb_config = {\n",
    "            #\"model_id\": cfg.model.model_id,\n",
    "            #\"lora_r\": cfg.lora.r,\n",
    "            #\"learning_rate\": cfg.training.learning_rate,\n",
    "            #\"batch_size\": cfg.training.per_device_train_batch_size,\n",
    "            #\"epochs\": cfg.training.num_train_epochs,\n",
    "            #\"gradient_accumulation_steps\": cfg.training.gradient_accumulation_steps,\n",
    "        #}\n",
    "        #wandb.init(\n",
    "            #project=cfg.wandb.project,\n",
    "            #name=cfg.wandb.name,\n",
    "            #config=wandb_config,\n",
    "            #settings=wandb.Settings(\n",
    "                #start_method=\"thread\"\n",
    "            #),  # Use thread-based initialization\n",
    "        #)\n",
    "\n",
    "instruction_template = \"user\\n\"\n",
    "response_template = \"model\\n\"\n",
    "collator = DataCollatorForCompletionOnlyLM(\n",
    "        instruction_template=instruction_template,\n",
    "        response_template=response_template,\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False,\n",
    "    )\n",
    "\n",
    "    # Initialize trainer\n",
    "trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        args=training_args,\n",
    "        peft_config=lora_config,\n",
    "        data_collator=collator,\n",
    "    )\n",
    "\n",
    "    # Add callbacks\n",
    "    #if env_vars[\"wandb_api_key\"]:\n",
    "        #trainer.add_callback(WandbLoggingCallback(trainer=trainer))\n",
    "if .1 > 0:\n",
    "    trainer.add_callback(EarlyStoppingCallback(early_stopping_patience=2))\n",
    "\n",
    "    # Start training\n",
    "trainer.train()\n",
    "\n",
    "    # Save the model\n",
    "final_model_path = f\"final\"\n",
    "trainer.save_model(final_model_path)\n",
    "\n",
    "    # Upload to Hugging Face Hub if repo_id is specified\n",
    "    #if hasattr(cfg, \"hub\") and cfg.hub.repo_id:\n",
    "        #if word:  # Only upload to subfolder if word was extracted\n",
    "            #upload_to_hub(\n",
    "                #final_model_path,\n",
    "                #f\"{cfg.hub.repo_id}-{word}\",\n",
    "                #word,\n",
    "                #env_vars[\"hf_token\"],\n",
    "            #)\n",
    "       # else:\n",
    "            # print(\"Skipping upload to subfolder due to extraction issue.\")\n",
    "             # Optionally, upload to base repo here if desired as a fallback\n",
    "             # upload_to_hub(final_model_path, cfg.hub.repo_id, None, env_vars[\"hf_token\"]) # Passing None or \"\" to subfolder might upload to root\n",
    "\n",
    "    # Finish wandb run if it was initialized\n",
    "    #if env_vars[\"wandb_api_key\"]:\n",
    "        #wandb.finish()\n",
    "\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "    #main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044a7eef-8ac8-43eb-96c6-069e663b7c0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
